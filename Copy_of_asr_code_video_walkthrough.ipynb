{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markuslyq/til-23-asr/blob/main/Copy_of_asr_code_video_walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init"
      ],
      "metadata": {
        "id": "-ddob1fBb6gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependencies"
      ],
      "metadata": {
        "id": "b6mDTzfOdZNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xptfr1qb3Z0",
        "outputId": "e408fce6-79fa-4dc8-9204-fb65b8a7c6d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03BU5MmYnTlG",
        "outputId": "85d6fb1b-b7af-46c1-e777-c88362a5ad48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm==4.65.0 in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer==3.0.1\n",
            "  Downloading jiwer-3.0.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer==3.0.1) (8.1.3)\n",
            "Collecting rapidfuzz==2.13.7 (from jiwer==3.0.1)\n",
            "  Downloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.1 rapidfuzz-2.13.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting librosa==0.9.1\n",
            "  Downloading librosa-0.9.1-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.1/213.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (1.2.0)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (4.4.2)\n",
            "Collecting resampy>=0.2.2 (from librosa==0.9.1)\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (23.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.45.1->librosa==0.9.1) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.45.1->librosa==0.9.1) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.9.1) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.9.1) (2.27.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->librosa==0.9.1) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.10.2->librosa==0.9.1) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.1) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (3.4)\n",
            "Installing collected packages: resampy, librosa\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.0.post2\n",
            "    Uninstalling librosa-0.10.0.post2:\n",
            "      Successfully uninstalled librosa-0.10.0.post2\n",
            "Successfully installed librosa-0.9.1 resampy-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==2.0.0rc\n",
            "  Downloading pandas-2.0.0rc0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.0rc) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.0rc) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.0rc) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.0rc) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.0.0rc0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.0.0rc0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting torch==1.12.1+cu116\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torch-1.12.1%2Bcu116-cp310-cp310-linux_x86_64.whl (1904.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 GB\u001b[0m \u001b[31m912.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.12.1\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torchaudio-0.12.1%2Bcu116-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu116) (4.5.0)\n",
            "Installing collected packages: torch, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.2+cu118\n",
            "    Uninstalling torchaudio-2.0.2+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.12.1+cu116 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.1+cu116 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.12.1+cu116 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.1+cu116 torchaudio-0.12.1+cu116\n"
          ]
        }
      ],
      "source": [
        "# installing dependencies, though some dependencies are built-in in colab, we want to make sure that the dependencies are the same for the student's environment as well\n",
        "!pip install tqdm==4.65.0\n",
        "!pip install jiwer==3.0.1   \n",
        "!pip install librosa==0.9.1\n",
        "!pip install pandas==2.0.0rc\n",
        "\n",
        "# download specific version of torch and torchaudio\n",
        "!pip install torch==1.12.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKc8dqBmEgQg",
        "outputId": "5e87dcdc-86f4-4a61-e59f-fd41379a883a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch                         1.12.1+cu116\n",
            "torchaudio                    0.12.1+cu116\n",
            "torchdata                     0.6.1\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.15.2\n",
            "torchvision                   0.15.2+cu118\n"
          ]
        }
      ],
      "source": [
        "# we only take account torch and torchaudio library here\n",
        "!pip list | grep torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "YneDnZ4uddHo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CsYRHrKAofo0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from jiwer import wer, cer\n",
        "from time import time\n",
        "import pandas as pd\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "\n",
        "# setting the random seed for reproducibility\n",
        "SEED = 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2A8vO7xTofrY"
      },
      "outputs": [],
      "source": [
        "class CustomSpeechDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    \"\"\"\n",
        "    Custom torch dataset class to load the dataset \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, manifest_file: str, audio_dir: str, is_test_set: bool=False) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        manifest_file: the csv file that contains the filename of the audio, and also the annotation if is_test_set is set to False\n",
        "        audio_dir: the root directory of the audio datasets\n",
        "        is_test_set: the flag variable to switch between loading of the train and the test set. Train set loads the annotation whereas test set does not\n",
        "        \"\"\"\n",
        "\n",
        "        self.audio_dir = audio_dir\n",
        "        self.is_test_set = is_test_set\n",
        "\n",
        "        self.manifest = pd.read_csv(manifest_file)\n",
        "\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        \n",
        "        \"\"\"\n",
        "        To get the number of loaded audio files in the dataset\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.manifest)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index: int) -> Tuple[str, torch.Tensor]:\n",
        "\n",
        "        \"\"\"\n",
        "        To get the values required to do the training\n",
        "        \"\"\"\n",
        "\n",
        "        if torch.is_tensor(index):\n",
        "            index.tolist()\n",
        "            \n",
        "        audio_path = self._get_audio_path(index)\n",
        "        signal, sr = torchaudio.load(audio_path)\n",
        "        \n",
        "        if not self.is_test_set:\n",
        "            annotation = self._get_annotation(index)\n",
        "            return audio_path, signal, annotation\n",
        "        \n",
        "        return audio_path, signal\n",
        "    \n",
        "    \n",
        "    def _get_audio_path(self, index: int) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        Helper function to retrieve the audio path from the csv manifest file\n",
        "        \"\"\"\n",
        "        \n",
        "        path = os.path.join(self.audio_dir, self.manifest.iloc[index]['path'])\n",
        "\n",
        "        return path\n",
        "    \n",
        "    \n",
        "    def _get_annotation(self, index: int) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        Helper function to retrieve the annotation from the csv manifest file\n",
        "        \"\"\"\n",
        "\n",
        "        return self.manifest.iloc[index]['annotation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g8Z415KnserF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Transforms text by encoding the characters and decoding the integers corresponding to the characters\n",
        "\"\"\"\n",
        "class TextTransform:\n",
        "\n",
        "    \"\"\"\n",
        "    Map characters to integers and vice versa (encoding/decoding)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        char_map_str = \"\"\"\n",
        "            <SPACE> 0\n",
        "            A 1\n",
        "            B 2\n",
        "            C 3\n",
        "            D 4\n",
        "            E 5\n",
        "            F 6\n",
        "            G 7\n",
        "            H 8\n",
        "            I 9\n",
        "            J 10\n",
        "            K 11\n",
        "            L 12\n",
        "            M 13\n",
        "            N 14\n",
        "            O 15\n",
        "            P 16\n",
        "            Q 17\n",
        "            R 18\n",
        "            S 19\n",
        "            T 20\n",
        "            U 21\n",
        "            V 22\n",
        "            W 23\n",
        "            X 24\n",
        "            Y 25\n",
        "            Z 26\n",
        "        \"\"\"\n",
        "        \n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        \n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "\n",
        "        self.index_map[0] = ' '\n",
        "\n",
        "\n",
        "    def get_char_len(self) -> int:\n",
        "\n",
        "        \"\"\"\n",
        "        Gets the number of characters that are being encoded and decoded in the prediction\n",
        "        Returns:\n",
        "        --------\n",
        "            the number of characters defined in the __init__ char_map_str\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.char_map)\n",
        "    \n",
        "\n",
        "    def get_char_list(self) -> List[str]:\n",
        "\n",
        "        \"\"\"\n",
        "        Gets the list of characters that are being encoded and decoded in the prediction\n",
        "        \n",
        "        Returns:\n",
        "        -------\n",
        "            a list of characters defined in the __init__ char_map_str\n",
        "        \"\"\"\n",
        "\n",
        "        return list(self.index_map.values())\n",
        "    \n",
        "\n",
        "    def text_to_int(self, text: str) -> List[int]:\n",
        "\n",
        "        \"\"\"\n",
        "        Use a character map and convert text to an integer sequence \n",
        "        Returns:\n",
        "        -------\n",
        "            a list of the text encoded to an integer sequence \n",
        "        \"\"\"\n",
        "        \n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "\n",
        "        return int_sequence\n",
        "    \n",
        "\n",
        "    def int_to_text(self, labels) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        Use a character map and convert integer labels to an text sequence \n",
        "        \n",
        "        Returns:\n",
        "        -------\n",
        "            the decoded transcription\n",
        "        \"\"\"\n",
        "        \n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "\n",
        "        return ''.join(string).replace('<SPACE>', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "azStvDMkstec"
      },
      "outputs": [],
      "source": [
        "class GreedyDecoder:\n",
        "\n",
        "    \"\"\"\n",
        "    Decodes the logits into characters to form the final transciption using the greedy decoding approach\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "\n",
        "    def decode(\n",
        "            self, \n",
        "            output: torch.Tensor, \n",
        "            labels: torch.Tensor=None, \n",
        "            label_lengths: List[int]=None, \n",
        "            collapse_repeated: bool=True, \n",
        "            is_test: bool=False\n",
        "        ):\n",
        "        \n",
        "        \"\"\"\n",
        "        Main method to call for the decoding of the text from the predicted logits\n",
        "        \"\"\"\n",
        "        \n",
        "        text_transform = TextTransform()\n",
        "        arg_maxes = torch.argmax(output, dim=2)\n",
        "        decodes = []\n",
        "\n",
        "        # refer to char_map_str in the TextTransform class -> only have index from 0 to 26, hence 27 represents the case where the character is decoded as blank (NOT <SPACE>)\n",
        "        decoded_blank_idx = text_transform.get_char_len()\n",
        "\n",
        "        if not is_test:\n",
        "            targets = []\n",
        "\n",
        "        for i, args in enumerate(arg_maxes):\n",
        "            decode = []\n",
        "\n",
        "            if not is_test:\n",
        "                targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\n",
        "            for j, char_idx in enumerate(args):\n",
        "                if char_idx != decoded_blank_idx:\n",
        "                    if collapse_repeated and j != 0 and char_idx == args[j-1]:\n",
        "                        continue\n",
        "                    decode.append(char_idx.item())\n",
        "            decodes.append(text_transform.int_to_text(decode))\n",
        "\n",
        "        return decodes, targets if not is_test else decodes\n",
        "    \n",
        "\n",
        "def BeamSearchDecoder(output, labels, label_lengths, blank_label=28, beam_width=10, \n",
        "                      max_len_ratio=2.0, collapse_repeated=True):\n",
        "    \"\"\"\n",
        "    Implements beam search decoding for a CTC model.\n",
        "    \n",
        "    Args:\n",
        "        output: The output of the CTC model.\n",
        "        labels: The ground truth labels.\n",
        "        label_lengths: The length of each ground truth label sequence.\n",
        "        blank_label: The label index used for the CTC blank symbol.\n",
        "        beam_width: The width of the beam to use during decoding.\n",
        "        max_len_ratio: The maximum allowed ratio of output length to input length.\n",
        "        collapse_repeated: Whether to collapse repeated characters in the output.\n",
        "    \n",
        "    Returns:\n",
        "        A list of decoded strings and a list of target strings.\n",
        "    \"\"\"\n",
        "    # Apply log softmax to the output to get log probabilities\n",
        "    log_probs = torch.nn.functional.log_softmax(output, dim=2)\n",
        "\n",
        "    # Get the batch size, sequence length, and number of characters in the vocabulary\n",
        "    batch_size, seq_len, vocab_size = log_probs.shape\n",
        "\n",
        "    # Initialize the beam for each example in the batch\n",
        "    beam = [(torch.tensor([blank_label]), 0.0)]\n",
        "\n",
        "    # Initialize the list of completed sequences\n",
        "    completed = []\n",
        "\n",
        "    # Loop over each time step in the sequence\n",
        "    for t in range(seq_len):\n",
        "        # Initialize a new beam\n",
        "        new_beam = []\n",
        "\n",
        "        # Loop over each item in the current beam\n",
        "        for seq, score in beam:\n",
        "            # Get the last character in the sequence\n",
        "            last_char = seq[-1]\n",
        "\n",
        "            # Loop over each character in the vocabulary\n",
        "            for c in range(vocab_size):\n",
        "                # Skip the blank label\n",
        "                if c == blank_label:\n",
        "                    continue\n",
        "\n",
        "                # Calculate the new score for the sequence\n",
        "                log_prob = log_probs[:, t, c]\n",
        "                new_seq = torch.cat((seq, torch.tensor([c])))\n",
        "                \n",
        "                new_score = score + log_prob.tolist()[0]\n",
        "\n",
        "                # If the new sequence is a repeat of the previous character, skip it\n",
        "                if collapse_repeated and c == last_char:\n",
        "                    continue\n",
        "\n",
        "                # If the new sequence is longer than the input sequence, skip it\n",
        "                if len(new_seq) > seq_len * max_len_ratio:\n",
        "                    continue\n",
        "\n",
        "                # If the new sequence is a valid target, add it to the list of completed sequences\n",
        "                if len(new_seq) == label_lengths[0] and torch.all(new_seq == labels[0][:label_lengths[0]]):\n",
        "                    completed.append((new_seq, new_score))\n",
        "                    continue\n",
        "\n",
        "                # Add the new sequence to the list of candidates\n",
        "                new_beam.append((new_seq, new_score))\n",
        "\n",
        "        # Sort the list of candidates by score and keep only the top k candidates\n",
        "        new_beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "        # Update the beam\n",
        "        beam = new_beam\n",
        "\n",
        "    # Sort the list of completed sequences by score and return the top sequence\n",
        "    completed = sorted(completed, key=lambda x: x[1], reverse=True)\n",
        "    if len(completed) > 0:\n",
        "        top_seq = completed[0][0]\n",
        "    else:\n",
        "        top_seq = beam[0][0]\n",
        "\n",
        "    # Convert the decoded sequence and target sequence to strings\n",
        "    decoded = text_transform.int_to_text(top_seq.tolist())\n",
        "    targets = text_transform.int_to_text(labels[0][:label_lengths[0]].tolist())\n",
        "\n",
        "    return [decoded], [targets]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7XW8xL6Ks2F4"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "\n",
        "    \"\"\"\n",
        "    Transforms the audio waveform tensors into a melspectrogram\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    def _audio_transformation(self, is_train: bool=True):\n",
        "\n",
        "        return torch.nn.Sequential(\n",
        "                torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "                torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "                torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        "            ) if is_train else torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
        "    \n",
        "\n",
        "    def data_processing(self, data, data_type='train'):\n",
        "\n",
        "        \"\"\"\n",
        "        Process the audio data to retrieve the spectrograms that will be used for the training\n",
        "        \"\"\"\n",
        "\n",
        "        text_transform = TextTransform()\n",
        "        spectrograms = []\n",
        "        input_lengths = []\n",
        "        audio_path_list = []\n",
        "\n",
        "        audio_transforms = self._audio_transformation(is_train=True) if data_type == 'train' else self._audio_transformation(is_train=False)\n",
        "\n",
        "        if data_type != 'test':  \n",
        "            labels = []\n",
        "            label_lengths = []\n",
        "\n",
        "            for audio_path, waveform, utterance in data:\n",
        "\n",
        "                spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "                spectrograms.append(spec)\n",
        "                label = torch.Tensor(text_transform.text_to_int(utterance))\n",
        "                labels.append(label)\n",
        "                input_lengths.append(spec.shape[0]//2)\n",
        "                label_lengths.append(len(label))\n",
        "\n",
        "            spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "            labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "            return audio_path, spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "        else:\n",
        "            for audio_path, waveform in data:\n",
        "\n",
        "                spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "                spectrograms.append(spec)\n",
        "                input_lengths.append(spec.shape[0]//2)\n",
        "                audio_path_list.append(audio_path)\n",
        "\n",
        "            spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "            return audio_path_list, spectrograms, input_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ISwmvDm3uJps"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "building the model with adaption of deepspeech2 -> https://arxiv.org/abs/1512.02595\n",
        "\n",
        "code adapted from https://towardsdatascience.com/customer-case-study-building-an-end-to-end-speech-recognition-model-in-pytorch-with-assemblyai-473030e47c7c\n",
        "\"\"\"\n",
        "\n",
        "class CNNLayerNorm(torch.nn.Module):\n",
        "    \n",
        "    \"\"\"\n",
        "    Layer normalization built for CNNs input\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_feats: int) -> None:\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "\n",
        "        self.layer_norm = torch.nn.LayerNorm(n_feats)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Input x of dimension -> (batch, channel, feature, time)\n",
        "        \"\"\"\n",
        "        \n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "\n",
        "class ResidualCNN(torch.nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel: int, stride: int, dropout: float, n_feats: int) -> None:\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = torch.nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = torch.nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = torch.nn.Dropout(dropout)\n",
        "        self.dropout2 = torch.nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \n",
        "        \"\"\"\n",
        "        Model building for the Residual CNN layers\n",
        "        \n",
        "        Input x of dimension -> (batch, channel, feature, time)\n",
        "        \"\"\"\n",
        "\n",
        "        residual = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(torch.nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    The Bidirectional GRU composite code block which will be used in the main SpeechRecognitionModel class\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, rnn_dim: int, hidden_size: int, dropout: int, batch_first: int) -> None:\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = torch.nn.GRU(\n",
        "            input_size=rnn_dim, \n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1, \n",
        "            batch_first=batch_first, \n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.layer_norm = torch.nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        \"\"\"\n",
        "        Transformation of the layers in the Bidirectional GRU block\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(torch.nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    The main ASR Model that the main code will interact with\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1) -> None:\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        \n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = torch.nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = torch.nn.Sequential(*[\n",
        "            ResidualCNN(\n",
        "                in_channels=32, \n",
        "                out_channels=32, \n",
        "                kernel=3, \n",
        "                stride=1, \n",
        "                dropout=dropout, \n",
        "                n_feats=n_feats\n",
        "            ) for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = torch.nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = torch.nn.Sequential(*[\n",
        "            BidirectionalGRU(\n",
        "                rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                hidden_size=rnn_dim, \n",
        "                dropout=dropout, \n",
        "                batch_first=i==0\n",
        "            ) for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        \"\"\"\n",
        "        Transformation of the layers in the ASR model block\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tUAlPWhTuJsI"
      },
      "outputs": [],
      "source": [
        "class IterMeter(object):\n",
        "\n",
        "    \"\"\"\n",
        "    Keeps track of the total iterations during the training and validation loop\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self) -> None:\n",
        "        self.val = 0\n",
        "\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "    \n",
        "\n",
        "class TrainingLoop:\n",
        "\n",
        "    \"\"\"\n",
        "    The main class to set up the training loop to train the model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "    \n",
        "\n",
        "    def train(self, model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        Training Loop\n",
        "        \"\"\"\n",
        "        \n",
        "        model.train()\n",
        "        data_len = len(train_loader.dataset)\n",
        "        \n",
        "        for batch_idx, _data in enumerate(train_loader):\n",
        "            start = time()\n",
        "            print(f\"{start}: training {batch_idx}\", end='\\r')\n",
        "            audio_path, spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            iter_meter.step()\n",
        "            \n",
        "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(spectrograms), data_len,\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "    def dev(self, model, device, dev_loader, criterion, scheduler, epoch, iter_meter) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        Validation Loop\n",
        "        \"\"\"\n",
        "        \n",
        "        print('\\nevaluating...')\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        test_cer, test_wer = [], []\n",
        "        greedy_decoder = GreedyDecoder()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i, _data in enumerate(dev_loader):\n",
        "                audio_path, spectrograms, labels, input_lengths, label_lengths = _data \n",
        "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                output = model(spectrograms)  # (batch, time, n_class)\n",
        "                output = F.log_softmax(output, dim=2)\n",
        "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                val_loss += loss.item() / len(dev_loader)\n",
        "\n",
        "                decoded_preds, decoded_targets = greedy_decoder.decode(output.transpose(0, 1), labels=labels, label_lengths=label_lengths, is_test=False)\n",
        "                print(decoded_preds)\n",
        "                print(decoded_targets)\n",
        "                \n",
        "                for j in range(len(decoded_preds)):\n",
        "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "        avg_cer = sum(test_cer)/len(test_cer)\n",
        "        avg_wer = sum(test_wer)/len(test_wer)\n",
        "        \n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print('Dev set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(val_loss, avg_cer, avg_wer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNPMngzjukzy"
      },
      "source": [
        "## Training a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "E9iMxERRumKx"
      },
      "outputs": [],
      "source": [
        "def main(hparams, train_dataset, dev_dataset, saved_model_path) -> None:\n",
        "\n",
        "    \"\"\"\n",
        "    The main method to call to do model training\n",
        "    \"\"\" \n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(SEED)\n",
        "    \n",
        "    data_processor = DataProcessor()\n",
        "    iter_meter = IterMeter()\n",
        "    text_transform = TextTransform()\n",
        "    trainer = TrainingLoop()\n",
        "    \n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=hparams['batch_size'],\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda x: data_processor.data_processing(x, 'train'),\n",
        "        **kwargs\n",
        "    )\n",
        "    \n",
        "    dev_loader = torch.utils.data.DataLoader(\n",
        "        dataset=dev_dataset,\n",
        "        batch_size=hparams['batch_size'],\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: data_processor.data_processing(x, 'dev'),\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], \n",
        "        hparams['n_rnn_layers'], \n",
        "        hparams['rnn_dim'],\n",
        "        hparams['n_class'], \n",
        "        hparams['n_feats'], \n",
        "        hparams['stride'], \n",
        "        hparams['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = torch.nn.CTCLoss(blank=text_transform.get_char_len()).to(device)\n",
        "    \n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', patience=3, verbose=True, factor=0.05)\n",
        "    \n",
        "    for epoch in range(1, hparams['epochs'] + 1):\n",
        "        trainer.train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n",
        "        trainer.dev(model, device, dev_loader, criterion, scheduler, epoch, iter_meter)\n",
        "        \n",
        "    # save the trained model\n",
        "    torch.save(model.state_dict(), saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nnQFAedu0k1",
        "outputId": "7f000479-8bc5-4228-c8d0-dfff43483b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3750\n",
            "3000\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Entry point of the code to do model training\n",
        "\"\"\"\n",
        "\n",
        "# change the filepath as according\n",
        "# MANIFEST_FILE_TRAIN = '/home/nicholas/datasets/til2023_asr_dataset/Train.csv'\n",
        "MANIFEST_FILE_TRAIN = '/content/drive/My Drive/Colab Notebooks/DSTA BrainHack ASR/Train.csv'\n",
        "# AUDIO_DIR_TRAIN = '/home/nicholas/datasets/til2023_asr_dataset/Train'\n",
        "AUDIO_DIR_TRAIN = '/content/drive/My Drive/Colab Notebooks/DSTA BrainHack ASR/DSTABrainHack2023/'\n",
        "# SAVED_MODEL_PATH = '/home/nicholas/models/til2023/model.pt\n",
        "SAVED_MODEL_PATH = '/content/drive/My Drive/Colab Notebooks/DSTA BrainHack ASR/model.pt'\n",
        "# MANIFEST_FILE_TRAIN = 'til2023_demo_dataset/train/annotation.json'\n",
        "# AUDIO_DIR_TRAIN = 'til2023_demo_dataset/train'\n",
        "# SAVED_MODEL_PATH = 'model.pt'\n",
        "\n",
        "# # simple check on the saved model path, will raise error if no directory found\n",
        "if not os.path.exists(os.path.dirname(SAVED_MODEL_PATH)):\n",
        "    raise FileNotFoundError\n",
        "\n",
        "# loads the dataset\n",
        "dataset = CustomSpeechDataset(\n",
        "    manifest_file=MANIFEST_FILE_TRAIN, \n",
        "    audio_dir=AUDIO_DIR_TRAIN, \n",
        "    is_test_set=False\n",
        ")\n",
        "print(len(dataset))\n",
        "# dataset = list(dataset)[:250]\n",
        "\n",
        "# train_dev_split\n",
        "train_proportion = int(0.8 * len(dataset))\n",
        "print(train_proportion)\n",
        "dataset_train = list(dataset)[:train_proportion]\n",
        "dataset_dev = list(dataset)[train_proportion:]\n",
        "\n",
        "\n",
        "hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 28, # 26 alphabets in caps + <SPACE> + blanks\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\": 2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": 1e-5,\n",
        "        \"batch_size\": 16,\n",
        "        \"epochs\": 50\n",
        "    }\n",
        "\n",
        "start_time = time()\n",
        "\n",
        "# start training the model\n",
        "main(\n",
        "    hparams=hparams, \n",
        "    train_dataset=dataset_train, \n",
        "    dev_dataset=dataset_dev, \n",
        "    saved_model_path=SAVED_MODEL_PATH\n",
        ")\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "print(f\"Time taken for training: {(end_time-start_time)/(60*60)} hrs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f88oeUIKwXA9",
        "outputId": "936a52a9-cf27-4c83-8712-771f63fe930f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "!cp model.pt \"content/drive/model.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J8Gi-U0wHzj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyisUG1ZwI2M"
      },
      "source": [
        "## Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm-x1lWrw3MW",
        "outputId": "705424a4-b985-477f-cbfd-ccb5f04790e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# downloading the model from gdrive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp \"/content/gdrive/My Drive/model.pt\" model.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVch4ZuiwLRp"
      },
      "outputs": [],
      "source": [
        "def infer(hparams, test_dataset, model_path) -> Dict[str, str]:\n",
        "    \n",
        "    print('\\ngenerating inference ...')\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(SEED)\n",
        "    \n",
        "    greedy_decoder = GreedyDecoder()\n",
        "    data_processor = DataProcessor()\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: data_processor.data_processing(x, 'test'),\n",
        "        **kwargs\n",
        "    )\n",
        "    \n",
        "    # load the pretrained model\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], \n",
        "        hparams['n_rnn_layers'], \n",
        "        hparams['rnn_dim'],\n",
        "        hparams['n_class'], \n",
        "        hparams['n_feats'], \n",
        "        hparams['stride'], \n",
        "        hparams['dropout']\n",
        "    ).to(device)\n",
        "    \n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    output_dict = {}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, _data in tqdm(enumerate(test_loader)):\n",
        "            audio_path, spectrograms, input_lengths = _data\n",
        "            spectrograms = spectrograms.to(device)\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class) \n",
        "            decoded_preds_batch = greedy_decoder.decode(output.transpose(0, 1), labels=None, label_lengths=None, is_test=True)\n",
        "            \n",
        "            # batch prediction\n",
        "            for decoded_idx in range(len(decoded_preds_batch[0])):\n",
        "                output_dict[audio_path[decoded_idx]] = decoded_preds_batch[0][decoded_idx]\n",
        "                \n",
        "    print('done!\\n')\n",
        "    return output_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApHKEicfwszv",
        "outputId": "69f872da-affc-47b5-878d-a3c6fdca6c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "generating inference ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.9/dist-packages/torchaudio/functional/functional.py:539: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n",
            "55it [00:10,  5.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "\n",
            "Time taken for inference: 0.18497053384780884 min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Entry point of the code to do model inference, also the code to use to generate the submission\n",
        "\"\"\"\n",
        "\n",
        "# same hyperparams as what you have used to train the model\n",
        "hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 28, # 26 alphabets in caps + <SPACE> + blanks\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\": 2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"batch_size\": 8,\n",
        "        \"epochs\": 15\n",
        "    }\n",
        "\n",
        "# change the filepath as according\n",
        "SAVED_MODEL_PATH = 'model.pt'\n",
        "SUBMISSION_PATH = 'submission.csv'\n",
        "\n",
        "MANIFEST_FILE_TEST = 'til2023_demo_dataset/interim/filenames.json'\n",
        "AUDIO_DIR_TEST = 'til2023_demo_dataset/interim/'\n",
        "\n",
        "dataset_test = CustomSpeechDataset(\n",
        "    manifest_file=MANIFEST_FILE_TEST, \n",
        "    audio_dir=AUDIO_DIR_TEST, \n",
        "    is_test_set=True)\n",
        "\n",
        "start_time = time()\n",
        "\n",
        "submission_dict = infer(\n",
        "    hparams=hparams, \n",
        "    test_dataset=dataset_test, \n",
        "    model_path=SAVED_MODEL_PATH\n",
        ")\n",
        "\n",
        "# producing the final csv file for submission\n",
        "submission_list = []\n",
        "\n",
        "for key in submission_dict:\n",
        "    submission_list.append(\n",
        "        {\n",
        "            \"path\": os.path.basename(key),\n",
        "            \"annotation\": submission_dict[key]\n",
        "        }\n",
        "    )\n",
        "\n",
        "submission_df = pd.DataFrame(submission_list)\n",
        "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "print(f\"Time taken for inference: {(end_time-start_time)/60} min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T37AjunvxcKz"
      },
      "outputs": [],
      "source": [
        "!cp submission.csv \"gdrive/My Drive/submission.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSKd2PNJSoXA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "collapsed_sections": [
        "-ddob1fBb6gs"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}